{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e63360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, Lasso\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from feature_selector import FeatureSelector\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel, SelectPercentile, f_classif\n",
    "from sklearn.utils import class_weight\n",
    "from matplotlib import rcParams\n",
    "from boruta import BorutaPy\n",
    "#from sklearn.externals import joblib\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476906a6",
   "metadata": {},
   "source": [
    "## Utility functions for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grid(clf_method):\n",
    "    try:\n",
    "        if clf_method == 'svm':\n",
    "            clf = svm.SVC()\n",
    "            params = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-2], 'C': [1, 5, 10, 100]},\n",
    "                      {'kernel': ['linear'], 'C': [1, 5, 10, 100]}]\n",
    "        \n",
    "        elif clf_method == 'randomforest':\n",
    "            clf = RandomForestClassifier()\n",
    "            params = [{\n",
    "                'n_estimators': [100, 150, 200],\n",
    "                'max_depth': [1, 2, 3],\n",
    "                'bootstrap': [True],\n",
    "                'max_features': ['auto', 'sqrt'],\n",
    "                'min_samples_split': [5, 10]\n",
    "                }] \n",
    "            \n",
    "        else:\n",
    "            print ('model type unkown!')\n",
    "            return None\n",
    "        \n",
    "        return (clf, params)\n",
    "        \n",
    "    except :\n",
    "        print ('exception occured in getting parameters!!')\n",
    "\n",
    "def perform_cv(X, y, sampling_method, ml_method, num_folds, random_state):\n",
    "    \n",
    "    clf, params = get_param_grid(ml_method)\n",
    "    cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)  \n",
    "    \n",
    "    # enumerate splits\n",
    "    fold_count = 0\n",
    "    fold_scores = []\n",
    "    pred_df = pd.DataFrame(index=X.index.values, columns=['truth', 'prediction'])\n",
    "    \n",
    "    \n",
    "    for train_ix, test_ix in cv_outer.split(X, y):\n",
    "                    \n",
    "        print('fold', fold_count)\n",
    "        X_train = X.iloc[train_ix]\n",
    "        X_test  = X.iloc[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # resample data\n",
    "        if sampling_method == 'ccmut':\n",
    "            print('\\nccmut sampling')\n",
    "            X_train, y_train = run_ccmut(X_train, y_train)\n",
    "        elif sampling_method == 'under+over':\n",
    "            print('\\nunder+over sampling')\n",
    "            X_train, y_train = under_and_over_sample(X_train, y_train)\n",
    "        else:\n",
    "            print('\\nno resampling')\n",
    "        \n",
    "        print('trainset distribution of classes:', y_train.value_counts().to_dict())\n",
    "        \n",
    "        # tune and train\n",
    "        print('tuning and training...')\n",
    "        search = GridSearchCV(clf, params, scoring=scoring, cv=cv_inner, refit=True)\n",
    "        gs = search.fit(X_train, y_train)\n",
    "        print('tuning done..')\n",
    "        best_clf = gs.best_estimator_\n",
    "        inner_score = gs.best_score_\n",
    "        print(gs.best_params_)\n",
    "\n",
    "        # test \n",
    "        print('predicting..')\n",
    "        yhat = best_clf.predict(X_test)\n",
    "        if scoring == 'f1_macro':\n",
    "            outer_score = f1_score(y_test, yhat, average='macro')\n",
    "        elif scoring == 'f1_weighted':\n",
    "            outer_score = f1_score(y_test, yhat, average='weighted')\n",
    "        elif scoring == 'f1':\n",
    "            outer_score = f1_score(y_test, yhat)\n",
    "        else:\n",
    "            outer_score = accuracy_score(y_test, yhat)\n",
    "        \n",
    "        \n",
    "        # report progress\n",
    "        print('inner %s=%.3f, outer %s=%.3f\\n' % (scoring, inner_score, scoring, outer_score))\n",
    "        fold_scores.append(outer_score)\n",
    "        fold_count += 1\n",
    "        pred_df.loc[X.index.values[test_ix], 'truth'] = y_test\n",
    "        pred_df.loc[X.index.values[test_ix], 'prediction'] = yhat\n",
    "        \n",
    "    return fold_scores, pred_df \n",
    "\n",
    "def train_on_all_data(X, y, sampling_method, ml_method):\n",
    "    \n",
    "    clf, params = get_param_grid(ml_method)\n",
    "    cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)  \n",
    "    \n",
    "    # resample data if specified\n",
    "    if sampling_method == 'ccmut':\n",
    "        print('ccmut sampling')\n",
    "        X_train, y_train = run_ccmut(X_train, y_train)\n",
    "    elif sampling_method == 'under+over':\n",
    "        print('under+over sampling')\n",
    "        X_train, y_train = under_and_over_sample(X_train, y_train)\n",
    "    else:\n",
    "        print('no resampling')\n",
    "\n",
    "    # tune and train\n",
    "    search = GridSearchCV(clf, params, scoring=scoring, cv=cv_inner, refit=True)\n",
    "    gs = search.fit(X, y)\n",
    "    best_clf = gs.best_estimator_\n",
    "    inner_score = gs.best_score_\n",
    "        \n",
    "    # report progress\n",
    "    print('best model, trained on all data, inner cv score f1=%.3f\\n' % inner_score)\n",
    "    \n",
    "    # return trained model\n",
    "    best_clf.fit(X, y)\n",
    "    return best_clf\n",
    "\n",
    "def run_repeated_cv(df, scale, fsmethod, sampling_method, ml_method, num_folds=5, num_trials=10):\n",
    "    \n",
    "    all_scores = []\n",
    "    all_trial_counts = []\n",
    "    all_fold_counts = []\n",
    "    for tr in range(num_trials):\n",
    "            \n",
    "        print('- trial %d' % (tr+1))\n",
    "\n",
    "        # dimensionality reduction, scaling & selecting features\n",
    "        select_df, _ = feature_preprocessing(df, scale, fsmethod)\n",
    "\n",
    "        # separate X and y\n",
    "        y = select_df.loc[:, targetcol]\n",
    "        X = select_df.drop(targetcol, axis=1)\n",
    "        \n",
    "        if X.shape[1] == 0:\n",
    "            print('--> no features passed feature preprocessing!')\n",
    "            continue\n",
    "\n",
    "        # run cv\n",
    "        scores, pred_df = perform_cv(X, y, sampling_method, ml_method, num_folds, tr)\n",
    "        all_scores += scores\n",
    "        all_trial_counts += [str(tr)] * num_folds\n",
    "        all_fold_counts += [str(i) for i in range(num_folds)]\n",
    "    \n",
    "    # save results\n",
    "    res_df = pd.DataFrame(index=np.arange(num_trials*num_folds))\n",
    "    res_df['score'] = all_scores\n",
    "    res_df['fold'] = all_fold_counts\n",
    "    res_df['trial'] = all_trial_counts\n",
    "    return res_df, pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1714b",
   "metadata": {},
   "source": [
    "## Utility functions for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91516ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations_df(df, labelvals):\n",
    "    \n",
    "    ftrnames = df.columns.values\n",
    "    temp_df = df.copy()\n",
    "    temp_df['target'] = labelvals\n",
    "    cors = temp_df.corr()\n",
    "\n",
    "    plt_df = pd.DataFrame(index=np.arange(len(ftrnames)),\n",
    "                          columns=['feature', 'correlation'])\n",
    "    plt_df['feature'] = ftrnames\n",
    "    plt_df['correlation'] = cors.loc[ftrnames, 'target'].values\n",
    "    \n",
    "    plt_df.dropna(inplace=True)\n",
    "    plt_df.sort_values(by=['correlation'], inplace=True)\n",
    "    plt_df.reindex(np.arange(len(ftrnames)))\n",
    "    return plt_df\n",
    "    \n",
    "def standardize_features(df, featurelist):\n",
    "    \"\"\"force features to have a 0 mean and unit std.\n",
    "    this doesnt create a bounding range for values. \n",
    "    returns scaled df and the scaler(for use on test set)\"\"\"\n",
    "    print('standardization feature scaling')\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    ftrVals = scaler.fit_transform(df)\n",
    "    out_df = pd.DataFrame(ftrVals, columns=featurelist)\n",
    "   \n",
    "    return out_df, scaler\n",
    "\n",
    "def normalize_features(df, featurelist):\n",
    "    \"\"\"force features to be between 0-1.\n",
    "    also knowns as min-max scaling. \n",
    "    returns scaled df and the scaler(for use on test set)\"\"\"\n",
    "    print('min-max feature scaling')\n",
    "    scaler = MinMaxScaler()\n",
    "    ftrVals = scaler.fit_transform(df)\n",
    "    out_df = pd.DataFrame(ftrVals, columns=featurelist)\n",
    "    return out_df, scaler\n",
    "\n",
    "def do_nothing_scaling(df, featurelist):\n",
    "    \n",
    "    \"\"\"this function exists for the sake of consistency and harmony\n",
    "    doesn't do anything\"\"\"\n",
    "    print('no feature scaling')\n",
    "    \n",
    "    out_df = pd.DataFrame(df, columns=featurelist)\n",
    "    return out_df, None\n",
    "\n",
    "def do_nothing_selection(df, y):\n",
    "    \n",
    "    print('no sampling')\n",
    "    return df.columns.values\n",
    "    \n",
    "def remove_collinear_features(df, labelvals, missing_threshold=0.5, correlation_threshold=0.75):\n",
    "    '''\n",
    "    removes overly missing and redundant features based on their collinearity\n",
    "    you can set what threshold to use for either one of the two.\n",
    "    '''\n",
    "    # remove redundant features\n",
    "    fs = FeatureSelector(data=df, labels=labelvals)\n",
    "    #fs.identify_missing(missing_threshold)\n",
    "    fs.identify_collinear(correlation_threshold)\n",
    "    \n",
    "    temp_df = fs.remove(methods=['collinear'])\n",
    "    print('%d non redundant features.' % len(temp_df.columns.values))\n",
    "    return temp_df\n",
    "\n",
    "def calculate_PCs(x, num):\n",
    "    \"\"\"Perform PCA on a list of features.\n",
    "\n",
    "    returns #num PC covariates and pc values.\n",
    "    \"\"\"\n",
    "    print ('calculating pcs..')\n",
    "    pca = PCA(n_components=num, svd_solver='full')\n",
    "    x_pca = pca.fit_transform(x)\n",
    "\n",
    "    res = {}\n",
    "    res['pca'] = pca\n",
    "    res['pc_vals'] = x_pca\n",
    "    \n",
    "    print('%d pcs were calculated to explain %.2f variance'%(x_pca.shape[1], n_components))\n",
    "    return res\n",
    "\n",
    "def select_features_boruta(df, y):\n",
    "    \"\"\"\n",
    "    Feature selection using random forest and burata method\n",
    "    Uses cross validation to find the best rf model and gets the most important features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe \n",
    "        the dataset. Note: all columns will be considered in feature selection\n",
    "    y : list\n",
    "        target variable for optimization.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    reduced list of features(length < df.shape[1]\n",
    "\n",
    "    \"\"\"\n",
    "    print('feature selection using random forest and boruta..')\n",
    "    \n",
    "    feat_labels = df.columns.values\n",
    "    \n",
    "    # define all parameters including max #features using a grid search\n",
    "    _, params = get_param_grid('randomforest')\n",
    "\n",
    "    # parameter grid search\n",
    "    gs = GridSearchCV(estimator=RandomForestClassifier(class_weight='balanced'), \n",
    "                      param_grid=params,\n",
    "                      n_jobs = -1,\n",
    "                      scoring=scoring,\n",
    "                      cv=5)\n",
    "    \n",
    "    gs.fit(df, y)\n",
    "    \n",
    "    print('best model:', gs.best_params_)\n",
    "    \n",
    "    # configure SelectFromModel to select features\n",
    "    # it needs a threshold for feature importance\n",
    "    # use the default mean for now, but this could be optimized with cv\n",
    "    \n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(gs.best_estimator_, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "    # find all relevant features\n",
    "    feat_selector.fit(np.array(df.values), y)\n",
    "    \n",
    "    # iterate through features and find selected ones\n",
    "    features = []\n",
    "    for f, rank, support in zip(feat_labels, feat_selector.ranking_, feat_selector.support_):\n",
    "        if support == True:\n",
    "            print('Feature: {:<25} Rank: {}'.format(f, rank))\n",
    "            features.append(f)\n",
    "    \n",
    "    print('\\n%d features were selected.'%len(features))\n",
    "    return features\n",
    "    \n",
    "def select_features_randomforest(df, y):\n",
    "    \"\"\"\n",
    "    Feature selection using random forest feature importance\n",
    "    Uses cross validation to find the best rf model and gets the most important features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe \n",
    "        the dataset. Note: all columns will be considered in feature selection\n",
    "    y : list\n",
    "        target variable for optimization.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    reduced list of features(length <df.shape[1]\n",
    "\n",
    "    \"\"\"\n",
    "    print('feature selection using random forest..')\n",
    "    \n",
    "    feat_labels = df.columns.values\n",
    "    \n",
    "    # define all parameters including max #features using a grid search\n",
    "    params = [{'n_estimators': [int(x) for x in np.linspace(start=50, stop=500, num=5)],\n",
    "               'max_depth': [int(x) for x in [3, 5, 7, 10]],\n",
    "               'bootstrap': [True]}]\n",
    "\n",
    "    # parameter grid search\n",
    "    gs = GridSearchCV(estimator=RandomForestClassifier(), \n",
    "                      param_grid=params,\n",
    "                      n_jobs = -1,\n",
    "                      scoring='f1_micro',\n",
    "                      cv=5)\n",
    "    \n",
    "    gs.fit(df, y)\n",
    "    \n",
    "    print('best model:', gs.best_params_)\n",
    "    \n",
    "    # configure SelectFromModel to select features\n",
    "    # it needs a threshold for feature importance\n",
    "    # use the default mean for now, but this could be optimized with cv\n",
    "    \n",
    "    print('select features using randomforest..')\n",
    "    best_rf = RandomForestClassifier(**gs.best_params_)\n",
    "    sfm = SelectFromModel(best_rf, threshold='median')\n",
    "    sfm.fit(df, y)\n",
    "    \n",
    "    # Print the names of the most important features\n",
    "    selected_ftrs = []\n",
    "    for ftr_ind in sfm.get_support(indices=True):\n",
    "        selected_ftrs.append(feat_labels[ftr_ind])\n",
    "        \n",
    "    print('%d features were selected by Random Forest.'%len(selected_ftrs))\n",
    "    return selected_ftrs\n",
    "\n",
    "\n",
    "def select_features_univariate(df, y, n=20):\n",
    "    \"\"\"Select features based on univariate F-test and 10 percentile.\n",
    "\n",
    "    normalized log scores will be used to reorder features.\n",
    "    return list of feature names that had a p-value less than alpha. \n",
    "    \"\"\"\n",
    "    print('select features based on univariate F-test..')\n",
    "    \n",
    "    ftr_indices = np.arange(df.shape[1])\n",
    "    feature_names = df.columns.values\n",
    "    \n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(df.values, y)\n",
    "    \n",
    "    scores = -np.log10(selector.pvalues_)\n",
    "    scores /= scores.max()\n",
    "    \n",
    "    alpha = 0.05 # pvalue threshold (ideally should be adjusted for multiple comparison)\n",
    "    indices_newOrder = [x for (y, x) in sorted(zip(selector.pvalues_, ftr_indices))]\n",
    "    indices_significant = [i for i in indices_newOrder if selector.pvalues_[i] < alpha]    \n",
    "    selected_ftrs = [feature_names[i] for i in indices_significant]\n",
    "    \n",
    "    print('%d features were selected using univariate analysis w threshold %.2f'%(len(selected_ftrs), alpha))\n",
    "    return selected_ftrs\n",
    "\n",
    "def select_features_coeffs(df, y):\n",
    "    \"\"\"\n",
    "    Select features using Lasso regression. \n",
    "    \n",
    "    parameter alpha defines how strict feature pruning is implemented.\n",
    "    Here we use cross validation to select best alpha value for cv performance\n",
    "    final alpha will be used to select features.\n",
    "    \"\"\"\n",
    "    print('select features using Lasso regression..')\n",
    "    # construct cv to find best alpha\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    model = LassoCV(cv=skf, normalize=True, max_iter=20000).fit(df, y)\n",
    "    print('best alpha:', model.alpha_)\n",
    "\n",
    "    # create a model with best alpha\n",
    "    print('fit data to a lasso model..')\n",
    "    clf = Lasso(alpha=model.alpha_)\n",
    "    coeffs = clf.fit(df, y).coef_\n",
    "\n",
    "    features = []\n",
    "    for f, c in zip(df.columns.values, coeffs):\n",
    "        if c != 0:\n",
    "            features.append(f)\n",
    "    \n",
    "    print('%d features were selected by Lasso.'%len(features))\n",
    "    return features\n",
    "\n",
    "def feature_preprocessing(df, scale, fs):\n",
    "        \n",
    "    othercols = []\n",
    "    \n",
    "    indices = df.index.values\n",
    "    \n",
    "    # get X\n",
    "    data_df = df[ftrcols].values\n",
    "\n",
    "    # get y, process y here right after (categorical to binary, etc)\n",
    "    y = df[targetcol].values\n",
    "    \n",
    "    # scale X \n",
    "    data_df, scaler = scaling_methods[scale](data_df, ftrcols)\n",
    "    \n",
    "    # add other columns to X (could be features that you want included, but dont want to scale)\n",
    "    if othercols:\n",
    "        for feature in othercols:\n",
    "            data_df[feature] = df[feature].values\n",
    "    \n",
    "    # impute missing values using median\n",
    "    data_df.fillna((data_df.median()), inplace=True)\n",
    "    \n",
    "    # remove redundant features\n",
    "    data_df = remove_collinear_features(data_df, y)\n",
    "    \n",
    "    # remove unimportant features\n",
    "    selected_features = fs_methods[fs](data_df, y)\n",
    "    \n",
    "    # get output\n",
    "    out_df = data_df[selected_features].copy()\n",
    "    out_df[targetcol] = y\n",
    "    out_df['index'] = indices   \n",
    "    out_df.set_index('index', inplace=True)\n",
    "    return out_df, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1402cb",
   "metadata": {},
   "source": [
    "## Utility functions for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76423b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CCMUT(df, f):\n",
    "    \"\"\"Cluster Centroif based majority UnderSampling Technique or CCMUT.\n",
    "    X: Matrix of Features from majority samples\n",
    "    f: Percentage of samples that should be removed.\n",
    "    \n",
    "    WORKS ONLY ON BINARY\n",
    "    \"\"\"\n",
    "    print('CCMUT..')\n",
    "    # keep track of the original index values\n",
    "    anon_ids = df.index.values\n",
    "    X = df.values\n",
    "    \n",
    "    # 1. find cluster centroid : average feature vectors\n",
    "    cluster_centroid = np.sum(X, axis=0) / X.shape[0]\n",
    "    \n",
    "    # 2. find Euclidean distance from cluster centroid to samples\n",
    "    euclidean = [None] * X.shape[0]\n",
    "    for i in range(0, X.shape[0]):\n",
    "        euclidean[i] = sqrt(sum((cluster_centroid - X[i]) ** 2))\n",
    "        \n",
    "    # 3. track indices of samples in descending order of distance\n",
    "    indices = list(reversed(sorted(range(len(euclidean)), key = lambda j: euclidean[j])))\n",
    "    \n",
    "    # 4. use the new order to rearrange anon ids\n",
    "    anon_ids_r = [anon_ids[i] for i in indices]\n",
    "    \n",
    "    # 5. remove the low score instances\n",
    "    X_f = np.delete(X, indices[:int(f * X.shape[0])], axis=0)\n",
    "    anon_ids_f = np.delete(anon_ids_r, indices[:int(f * X.shape[0])], axis=0)\n",
    "    \n",
    "    # 5. return the under-sampled Majority Sample Matrix\n",
    "    return pd.DataFrame(index=anon_ids_f, data=X_f, columns=df.columns.values)\n",
    "\n",
    "def run_ccmut(df, y):\n",
    "    \"\"\"\n",
    "    ONLY FOR BINARY DATA\n",
    "    df is train dataframe, y is y_train:\n",
    "    \n",
    "    - identify majority and minority groups\n",
    "    - undersample majority\n",
    "    - merge with minority again\n",
    "    - return balanced trainset dataframe\"\"\"\n",
    "    \n",
    "    # identify majority and minority labels and counts\n",
    "    counts = y.value_counts().to_dict()\n",
    "    \n",
    "    [(min_class, min_n), (maj_class, maj_n)] = sorted(counts.items(), key=lambda x: x[1], reverse=False)\n",
    "    print(min_class, min_n, '-', maj_class, maj_n)\n",
    "    \n",
    "    # measure % to be removed from majority\n",
    "    f = float((maj_n - min_n) / maj_n)\n",
    "    \n",
    "    # skip resampling if class imbalance is negligible\n",
    "    if f <= 0.1:\n",
    "        print('classes are balanced (f = %.3f). skip resampling.' % f)\n",
    "        return df, y\n",
    "\n",
    "    print('%.3f perc of samples from group %d will be droped..' % (f, maj_class))\n",
    "                \n",
    "    # separate majority and minority dfs\n",
    "    maj_ids = y[y == maj_class].index.values\n",
    "    min_ids = y[y == min_class].index.values\n",
    "    \n",
    "    print('overlap of indices between min and maj groups', len([a for a in maj_ids if a in min_ids]))\n",
    "    \n",
    "    # run CCMUT on majority\n",
    "    maj_r_df = CCMUT(df.loc[maj_ids, :], f)\n",
    "    maj_r_df.set_index(maj_r_df.index.values, inplace=True)\n",
    "    print('majority df after ccmut', maj_r_df.shape, 'has nans:', maj_r_df.isnull().values.any())\n",
    "    \n",
    "    # separate minority\n",
    "    min_df = df.loc[min_ids, :]\n",
    "    min_df.set_index(min_ids, inplace=True)\n",
    "    print('minority df', min_df.shape, 'has nans:', min_df.isnull().values.any())\n",
    "    \n",
    "    # merge \n",
    "    df_r = pd.concat([maj_r_df, min_df])\n",
    "    print('\\nfinal df :', df_r.shape, 'has nans:', df_r.isnull().values.any())\n",
    "  \n",
    "    y_r = pd.concat([y[maj_r_df.index.values], y[min_ids]])\n",
    "    print('final y :', y_r.value_counts().to_dict())\n",
    "    \n",
    "    # return merged df and labels\n",
    "    return df_r , y_r\n",
    "\n",
    "def sampling_strategy(X,y,n_samples, t='majority'):\n",
    "    '''not my code. Got it from:\n",
    "    https://towardsdatascience.com/how-to-deal-with-imbalanced-multiclass-datasets-in-python-fe0bb3f2b669\n",
    "    '''\n",
    "    target_classes = ''\n",
    "    if t == 'majority':\n",
    "        target_classes = y.value_counts() > n_samples\n",
    "    elif t == 'minority':\n",
    "        target_classes = y.value_counts() < n_samples\n",
    "    tc = target_classes[target_classes == True].index\n",
    "    \n",
    "    sampling_strategy = {}\n",
    "    for target in tc:\n",
    "        sampling_strategy[target] = n_samples\n",
    "    return sampling_strategy\n",
    "\n",
    "def under_and_over_sample(X, y):\n",
    "    \n",
    "    count = y.value_counts()\n",
    "    n_samples = int(count.median())\n",
    "    \n",
    "    print('counts before resampling:', count.values)\n",
    "\n",
    "    under_sampler = ClusterCentroids(sampling_strategy=sampling_strategy(X, y, n_samples, t='majority'))\n",
    "    X_under, y_under = under_sampler.fit_resample(X, y)\n",
    "\n",
    "    \n",
    "    \n",
    "    over_sampler = SMOTE(sampling_strategy=sampling_strategy(X_under, y_under, n_samples, t='minority'), \n",
    "                         k_neighbors=2)\n",
    "    X_bal, y_bal = over_sampler.fit_resample(X_under, y_under)\n",
    "    count = y_bal.value_counts()\n",
    "    print('counts after resampling:', count.values, '\\n')\n",
    "    \n",
    "    return X_bal, y_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54052f2",
   "metadata": {},
   "source": [
    "# main "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6684594",
   "metadata": {},
   "source": [
    "#### select settings that you'd like to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e42369",
   "metadata": {},
   "outputs": [],
   "source": [
    "## options for feature processing and feature selection\n",
    "\n",
    "# feature scaling options\n",
    "scaling_methods = {'none': do_nothing_scaling,\n",
    "                   'normalize': normalize_features,\n",
    "                   'standardize': standardize_features}\n",
    "\n",
    "# feature selection options\n",
    "fs_methods = {'rf':select_features_randomforest,\n",
    "              'lasso':select_features_coeffs,\n",
    "              'univariate':select_features_univariate,\n",
    "              'boruta':select_features_boruta,\n",
    "              'none':do_nothing_selection}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings that we will use (you can select from the above lists)\n",
    "# this will define the folder name\n",
    "\n",
    "compute_pca = False \n",
    "scaling_method = 'none'\n",
    "featureselection_method = 'none'\n",
    "sampling_method = 'under+over'\n",
    "ml_method = 'randomforest'\n",
    "global scoring\n",
    "scoring = 'f1_weighted' # can also try f1_weighted, f1_macro, accuracy\n",
    "num_folds = 5\n",
    "num_trials = 10 # for repeated cross validation to get a more accurate estimate of performance\n",
    "\n",
    "runname = '-'.join([s for s in [scaling_method,featureselection_method,sampling_method,ml_method,scoring]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223106c9",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = 'classification-BulkCluster'\n",
    "\n",
    "global targetcol\n",
    "targetcol = 'BulkCluster'\n",
    "\n",
    "# disable warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "# paths\n",
    "csvname = '__.csv'\n",
    "input_dir = os.getcwd() \n",
    "csvpath = os.path.join(input_dir, csvname)\n",
    "output_dir = os.path.join(input_dir, foldername, runname)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print('input_dir:', input_dir)\n",
    "print('output_dir:', output_dir)\n",
    "\n",
    "# create sex specific folders for outputs\n",
    "cohorts = ['all', 'M', 'F']\n",
    "for sex in cohorts:\n",
    "    sex_spec_output_dir = os.path.join(output_dir, sex)\n",
    "    if not os.path.exists(sex_spec_output_dir):\n",
    "        os.makedirs(sex_spec_output_dir)\n",
    "\n",
    "# read dataframe\n",
    "indexcol = 'SampleID'\n",
    "sexcol = 'Sex'\n",
    "global ftrcols\n",
    "ftrcols = ['Enhancing','RCBV.Raw_Mean', 'RCBV.Raw_Std', 'FA.Raw_Std', \n",
    "           'FA.Raw_Mean', 'MD.Raw_Mean', 'MD.Raw_Std', 'EPI.Raw_Mean', \n",
    "           'EPI.Raw_Std', 'CenterFecsT2', 'MeanFecsT2', 'StdFecsT2']\n",
    "df = pd.read_csv(csvpath)\n",
    "\n",
    "df[targetcol].replace(['A', 'B', 'C'], [0, 1, 2], inplace=True)\n",
    "df['Enhancing'].replace(['ENH', 'BAT', 'NEC', 'Central Cyst/Cavity'], [0, 1, 2, 3], inplace=True)\n",
    "\n",
    "# exclude additional columns\n",
    "df = df.loc[:, ftrcols+[sexcol, targetcol, indexcol]]\n",
    "df.set_index(indexcol, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0954e",
   "metadata": {},
   "source": [
    "### ----- repeated cv for evaluation of machine learning performance -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ffca5",
   "metadata": {},
   "source": [
    "#### step 1 - training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4531e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run modeling for cohorts\n",
    "warnings.filterwarnings('ignore')\n",
    "for sex in cohorts:\n",
    "    print('\\n', '-'*10, sex, '-'*10)\n",
    "\n",
    "    sexed_input_dir = os.path.join(output_dir, sex)\n",
    "    \n",
    "    results_dir = os.path.join(sexed_input_dir, 'ml_results')\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    savecvpredpath = os.path.join(results_dir, 'cvpredictions.csv') \n",
    "    \n",
    "    savecvrespath = os.path.join(results_dir, 'cvresult.csv') \n",
    "    if os.path.exists(savecvrespath):\n",
    "        print('cvresult.csv exists!')\n",
    "        continue\n",
    "        \n",
    "    # select cohort\n",
    "    if sex == 'all': \n",
    "        sex_df = df.copy()\n",
    "    else: \n",
    "        sex_df = df[df[sexcol] == sex]\n",
    "\n",
    "    results_df, prediction_df = run_repeated_cv(sex_df, scaling_method, featureselection_method, \n",
    "                                                sampling_method, ml_method, num_folds, num_trials)\n",
    "    \n",
    "    results_df.to_csv(savecvrespath, index=False)\n",
    "    prediction_df.to_csv(savecvpredpath, index=True)\n",
    "    \n",
    "    print(savecvrespath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b7bba1",
   "metadata": {},
   "source": [
    "#### step 2 - merge all results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609be97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results\n",
    "merged_results_df = pd.DataFrame(columns=['cohort', 'fold', 'trial', 'score'],\n",
    "                                 index=np.arange(num_folds*num_trials*len(cohorts)))\n",
    "print(merged_results_df.shape)\n",
    "start_index = 0\n",
    "for sex in cohorts:\n",
    "    print('merging %s' % sex)\n",
    "    csvpath = os.path.join(output_dir, sex, 'ml_results', 'cvresult.csv')\n",
    "    df = pd.read_csv(csvpath)\n",
    "    \n",
    "    end_index = start_index + num_folds * num_trials - 1\n",
    "    print(df.shape,start_index, end_index)\n",
    "    \n",
    "    for col in df.columns.values:\n",
    "        merged_results_df.loc[start_index:end_index, col] = df[col].values\n",
    "    \n",
    "    merged_results_df.loc[start_index:end_index, 'cohort'] = sex\n",
    "    start_index = end_index\n",
    "\n",
    "merged_results_df.dropna(inplace=True)\n",
    "\n",
    "# plot\n",
    "fig_save_dir = output_dir\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "fig = plt.figure(figsize=(len(cohorts)*2, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax = sns.boxplot(y='score', x='cohort', data=merged_results_df, palette='Blues')\n",
    "plt.ylabel(scoring, fontsize=12)\n",
    "plt.xlabel('cohort', fontsize=12)\n",
    "plt.title('random forest performance in predicting %s'%targetcol, fontsize=15)\n",
    "ax.set_ylim([0., 1.0])\n",
    "plt.show()\n",
    "    \n",
    "fig.savefig(os.path.join(fig_save_dir, 'cv_summary.jpg'), bbox_inches='tight', pad_inches=1, dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5513a",
   "metadata": {},
   "source": [
    "#### plot per class classification performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615a2af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "def accuracy (truth, pred):\n",
    "    correct = sum([1 for i in range(len(truth)) if pred[i] == truth[i]])\n",
    "    return float(correct) / len(truth)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(len(cohorts)*4, 3))\n",
    "\n",
    "for n, sex in enumerate(cohorts):\n",
    "    ax = fig.add_subplot( 1, 3, n+1)\n",
    "    predictioncsvpath = os.path.join(output_dir, sex, 'ml_results','cvpredictions.csv')\n",
    "    pred_df = pd.read_csv(predictioncsvpath)\n",
    "    \n",
    "    y_test = list(pred_df['truth'].values)\n",
    "    y_pred = list(pred_df['prediction'].values)\n",
    "    ax.set_title('%s samples\\noverall perc acc = %.1f' % (sex, accuracy(y_test, y_pred) * 100))\n",
    "\n",
    "        \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['A', 'B', 'C'])\n",
    "    disp = disp.plot(include_values=True, cmap='Purples', xticks_rotation='horizontal', ax=ax, colorbar='')\n",
    "    plt.grid(b=None)\n",
    "    \n",
    "    ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "plt.show()\n",
    "    \n",
    "fig.savefig(os.path.join(fig_save_dir, 'confmat.jpg'), bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
    "\n",
    "plt.close('all')\n",
    "    \n",
    "t1 = time()\n",
    "print('\\n\\n------------time taken for this run:', t1 - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
